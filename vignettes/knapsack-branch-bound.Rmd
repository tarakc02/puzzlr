---
title: "Solving the knapsack with branch-and-bound"
author: "Tarak Shah"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Solving the knapsack with branch-and-bound}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

## The Knapsack problem

The [knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem), in its simplest form, asks: given a sack with a maximum carrying capacity, and a set of items of varying weights and values, take a subset of the items that has the highest total value but that still fits in the sack. The puzzlr package includes functions to create instances of the knapsack problem:

```{r}
library(puzzlr)
set.seed(97317)
ks <- weakly_correlated_instance(n = 50)
ks
```

Each instance consists of a `capacity` and a set of `items`:

```{r}
capacity(ks)
items(ks)
```

By default, items in knapsack instances are ordered in decreasing order of value per unit weight, or density. The top item for a given knapsack instance can be accessed with the `next_item` function:

```{r}
next_item(ks)
```

Given a knapsack instance, a new instance can be created by either taking or leaving the next item, leaving a sub problem:

```{r}
take_next(ks)
leave_next(ks)
sub_problems(ks)
```

In this way, one can exhaustively enumerate all subsets of items: with the original problem as a root node, create two branches based on the subproblems `take_next(ks)` and `leave_next(ks)`, and then construct each of their two subproblems, and so on until there are no more items left to consider. The optimal solution is found in one of the leaf nodes of the tree -- among those leaf nodes whose aggregate weight does not exceed `capacity(ks)`, it is the one that maximizes the total value. Given n items, there are $2^n$ leaf nodes. So how can one find the optimal solution in a reasonable amount of time?

## Branch and bound

[Branch and bound](https://en.wikipedia.org/wiki/Branch_and_bound) is a family of algorithms for finding the provably optimal solution of a problem like the knapsack that can be represented in terms of branching sub-problems. In the case of the knapsack problem, given a fast way to estimate upper and lower bounds on the true optimum for a sub-problem, prune the search tree during its construction: 

- Keep track of the best lower bound seen so far. 
- When branching, if one of the subproblems has an upper bound that is lower than the best lower bound seen so far, prune the tree -- there is no need to continue exploring that path.

The lazily evaluated lists from the [lazylist](https://github.com/tarakc02/lazylist) package provide an appealing way to implement branch-and-bound, due to the ability to define data structures recursively. I'll start with the high-level implementation and then fill in the details. I use the `empty_stream()` to prune or terminate a search path, and combine multiple search paths into one by using a `search_strategy` that prioritizes search nodes:

```{r}
library(lazylist)
solution_tree <- function(root, 
                          branch,
                          search_strategy, 
                          best_so_far) {
    branches <- branch(root)
    if (length(branches) == 0) return(empty_stream())
    
    explore_and_prune <- function(node) {
        if (upper_bound(node) < best_so_far) return(empty_stream())
        new_best <- max(best_so_far, lower_bound(node))
        cons_stream(node,
                    solution_tree(node,
                                  branch = branch,
                                  search_strategy = search_strategy,
                                  best_so_far = new_best))
    }
    branches <- purrr::map(branches, explore_and_prune)
    purrr::reduce(branches, search_strategy)
}
```

## Search strategy

A search strategy should be a function that takes two lazy-lists (or "streams") and combines them into one. Since it's being applied repeatedly at every branch, I like to visualize its effect as braiding or interleaving all of the branches together. The result is a single stream of search nodes, with ordering dependent on the search strategy. There are multiple search strategies I might implement this way: breadth-first, depth-first, etc. As a first example, I'll implement best-first search, in which, at each stage of the search, the "best" node is considered, with "best" defined as the node with the highest `upper_bound`. Again, the ability to define things recursively helps -- `best_first` is defined as taking the better of the two head elements of two streams, along with the `best_first` of the remaining elements of both streams:

```{r}
best_first <- function(s1, s2) {
    if (is_emptystream(s1)) return(s2)
    if (is_emptystream(s2)) return(s1)
    node1 <- stream_car(s1)
    node2 <- stream_car(s2)
    if (upper_bound(node1) >= upper_bound(node2)) {
        return(cons_stream(node1, 
                           best_first(stream_cdr(s1), s2) ))
    }
    cons_stream(node2, 
                best_first(s1, stream_cdr(s2)) )
}
```

## Search nodes and branching

I still need a way to create search nodes that have `upper_bound` and `lower_bound` methods. I'll define a search node as a knapsack instance with some additional information tacked on.

```{r}
search_node <- function(ks, bounds) {
    b <- bounds(ks)
    list(problem = ks, 
         upper_bound = b$upper_bound,
         lower_bound = b$lower_bound)
}
upper_bound <- function(node) node$upper_bound
lower_bound <- function(node) node$lower_bound
```

## Greedy: a simple bounding function

I've so far been working under the assumption that there is a quick way to estimate the upper and lower bounds of a knapsack instance. There are various trivial bounds to a problem, for instance a lower bound of 0, or an upper bound equal to the sum of values of all of items. I'm looking for two qualities from a bounding function, and they are in conflict with one another:

* it should be fast to calculate, since it's calculated for every node in the search tree
* it should give tight bounds: for a given node, a tighter upper bound makes it more likely to be pruned, while a tighter lower bound makes all other nodes more likely to be pruned. The more nodes that are pruned, the quicker the search.

Since items are already sorted in order of density, I can take items one at a time until the knapsack is full, giving a decent lower bound. To get an upper bound, I consider an easier to solve problem: what is the most value possible if I'm allowed to take fractional items? Taking items in order of density and then taking a fraction of the next item will result in a total value that is no less than the true optimum. The `greedy` function calculates both bounds:

```{r}
greedy <- function(ks) {
    item <- next_item(ks)
    
    # if the only remaining item doesn't fit
    if (n_items(ks) == 1 && item$weight > capacity(ks)) return(
        list(lower_bound = total_value(ks),
             upper_bound = total_value(ks),
             solution = leave_next(ks))
    )
    
    solution <- ks
    while (!is.null(item) && 
           item$weight <= capacity(solution)) {
        solution <- take_next(solution)
        item <- next_item(solution)
    }
    lower_bound <- total_value(solution)
    
    upper_bound <- lower_bound
    if (capacity(solution) > 0 && !is.null(item)) {
        partial_amount <- capacity(solution) / item$weight
        upper_bound <- upper_bound + (partial_amount * item$value)
    }
    
    list(lower_bound = lower_bound,
         upper_bound = upper_bound,
         solution = solution)
}

# the root of the search tree for ks:
search_node(ks, bounds = greedy)
```

## Generating the solution

I now have all of the necessary components of a knapsack solver:

```{r}
solve_knapsack <- function(ks, bounds, search_strategy) {
    root <- search_node(ks, bounds = bounds)
    # branch function takes a search node and returns its children
    branch <- function(node) {
        subprobs <- sub_problems(node$problem)
        purrr::map(subprobs, search_node, bounds = bounds)
    }
    
    cons_stream(root, 
                solution_tree(root, 
                              branch = branch,
                              search_strategy = search_strategy,
                              best_so_far = root$lower_bound))
}

solution <- solve_knapsack(ks, bounds = greedy, search_strategy = best_first)
solution[50]
```

Recall that the leaf nodes collectively hold all possible solutions. Since I'm using best first search, the first leaf-node that is processed must be the optimal solution. If there were a better solution than the first leaf node, it would have to have a better upper bound, so it would have been processed earlier.

A leaf node is one where the subproblem has no remaining items to consider.

```{r}
optimal <- stream_which(solution, 
                        function(x) n_items(x$problem) == 0)
optimal <- optimal[1]
optimal
solution[optimal]
# view which items were taken
taken_items(solution[optimal]$problem)
```

## Analyzing the algorithm

The `solution` object contains the full search history, starting from the original problem at `solution[1]` to the solution on step `r format(optimal, big.mark = ",")`.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
solution_df <- data_frame(
    timestamp = seq_len(optimal),
    lower_bound = stream_map(solution, lower_bound) %>% 
        as.list(from = 1, to = optimal) %>% as.numeric,
    upper_bound = stream_map(solution, upper_bound) %>%
        as.list(from = 1, to = optimal) %>% as.numeric) %>%
    mutate(best_seen = cummax(lower_bound))
solution_df
```

I can see progress towards the solution:

```{r}
theme_set(hrbrthemes::theme_ipsum())

solution_df %>%
    select(-lower_bound) %>%
    gather(series, value, -timestamp) %>%
    ggplot(aes(x = timestamp, y = value)) +
    geom_line(aes(linetype = series)) +
    geom_hline(yintercept = total_value(solution[optimal]$problem),
               colour = "red", linetype = 4) +
    scale_y_continuous(labels = scales::comma)
```

*Note that since I used best-first search, I could have stopped as soon as the `lower_bound` and `upper_bound` were equal, though that's not generally true for other search strategies.*

We don't have to run `solution` until a provably optimal solution -- we can stop earlier if we like. If we did so, we'd return the `best_seen` value as the best we could do. In fact, we can use calculate the best seen percent of upper bound at each step, and so even if we stopped early, we'd be able to report a guarantee on how close the given solution is to optimal (even though we wouldn't know the true optimal solution). For instance, by time 58, we know that our best solution is at least `r scales::percent(solution_df$best_seen[58]/solution_df$upper_bound[58])` of the true optimum. 

```{r}
solution_df %>%
    mutate(pct_of_upper_bound = best_seen / upper_bound) %>%
    ggplot(aes(x = timestamp, y = pct_of_upper_bound)) +
    geom_line()
```

## Detour: an exact solution

In addition to branch-and-bound based solutions, there are a couple of different [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) based solutions to the knapsack. The one I'll explore here breaks the problem down into sub-problems in a somewhat unintuitive, but ultimately useful way. First, we re-frame the question: for a given value `v`, what weight is required in order to have items whose values add up to exactly `v` (if there are multiple ways to get to `v`, then give the minimum weight necessary)? We'll create a function `min_wt(n, v)` to answer that question.

I can work out a recursive solution. Assume we know the value of `min_wt` for numbers less than `n`. Then I'm left looking at the nth item. The solution to `min_wt(n, v)` either includes the nth item, in which case I add the weight of the nth item to the solution of the resulting subproblem (here `value[n]` is the value of the nth item, and `weight[n]` is its weight):

```{r, eval = FALSE}
min_wt(n, v) == weight[n] + min_wt(n - 1, v - value[n])
```

Or does not include the nth item, in which case:

```{r, eval = FALSE}
min_wt(n, v) == min_wt(n - 1, v)
```

Whether to include or exclude the nth item depends on which of those two expressions evaluates to a smaller amount (because we seek the *minimum* capacity required to get to `v`). Putting those ideas together in a function, I get:

```{r, eval = FALSE}
exact_solution <- function(ks) {
    num_items <- n_items(ks); items <- items(ks)
    capacity <- capacity(ks); weight <- items$weight; value <- items$value
    
    min_wt <- function(n, v) {
        if (v == 0) return(0)
        if (n == 0) return(Inf)
        
        if (value[n] > v) {
            min_wt(n - 1, v)
        } else {
            min(min_wt(n - 1, v),
                weight[n] + min_wt(n - 1, v - value[n]))
        }
    }
}
```

But what does any of this have to do with solving the knapsack? Well, if I know some maximum value that is achievable in this instance of the knapsack (an upper bound, that is), then I can try `min_wt(n_items(ks), k)` for decreasing values of `k` until I find one whose result is less than or equal to `capacity(ks)`, thus discovering the optimal value for the knapsack (here `num_items` is the number of items in the instance, and `capacity` is its capacity):

```{r, eval = FALSE}
k <- max_value(ks)
while (min_wt(num_items, k) > capacity) k <- k - 1
```

I can use `greedy` to find an upper bound (the `max_value`). My overall function now looks like:

```{r}
exact_solution <- function(ks) {
    num_items <- n_items(ks); items <- items(ks)
    capacity <- capacity(ks); weight <- items$weight; value <- items$value
    max_value <- greedy(ks)$upper_bound
    
    min_wt <- function(n, v) {
        if (v == 0) return(0)
        if (n == 0) return(Inf)
        
        if (value[n] > v) {
            min_wt(n - 1, v)
        } else {
            min(min_wt(n - 1, v),
                weight[n] + min_wt(n - 1, v - value[n]))
        }
    }
    
    k <- max_value
    while (min_wt(num_items, k) > capacity) k <- k - 1
    k
}
```

There's still a problem: I'm re-calculating `min_wt` over and over for the same inputs. To get the value of `min_wt(n, v)` I need to calculate `min_wt(n - 1, v)` as well as `min_wt(n - 1, v - value[n])` and so on for each of those. This is an instance where caching, or memoization, can make a big difference. Since `min_wt` has two integer arguments and an integer output, I can use a matrix to store already-calculated values -- looking up a single value in a matrix can be done very quickly:

```{r}
exact_solution <- function(ks) {
    num_items <- n_items(ks); items <- items(ks)
    capacity <- capacity(ks); weight <- items$weight; value <- items$value
    max_value <- floor(greedy(ks)$upper_bound)
    
    cache <- matrix(nrow = num_items, ncol = max_value, data = NA_integer_)
    
    min_wt <- function(n, v) {
        if (v == 0) return(0)
        if (n == 0) return(Inf)
        if (!is.na(cache[n, v])) return(cache[n, v])
        
        if (value[n] > v) {
            res <- min_wt(n - 1, v)
        } else {
            res <- min(min_wt(n - 1, v),
                       weight[n] + min_wt(n - 1, v - value[n]))
        }
        cache[n, v] <<- res
        res
    }
    
    k <- max_value
    while (min_wt(num_items, k) > capacity) k <- k - 1
    k
}
exact_solution(ks)
```

I can verify that that is the optimum by comparing to the solution from the previous section (yes, it is!), but how can I get see the items taken to get to that value? I can use the same logic I used in creating the `min_wt` function, but backwards: if `min_wt(n + 1, v)` is different from `min_wt(n, v)` that means I must have taken item `n + 1`:

```{r}
exact_solution <- function(ks) {
    num_items <- n_items(ks); items <- items(ks)
    capacity <- capacity(ks); weight <- items$weight; value <- items$value
    max_value <- floor(greedy(ks)$upper_bound)
    
    cache <- matrix(nrow = num_items, ncol = max_value, data = NA_integer_)
    
    min_wt <- function(n, v) {
        if (v == 0) return(0)
        if (n == 0) return(Inf)
        if (!is.na(cache[n, v])) return(cache[n, v])
        
        if (value[n] > v) {
            res <- min_wt(n - 1, v)
        } else {
            res <- min(min_wt(n - 1, v),
                       weight[n] + min_wt(n - 1, v - value[n]))
        }
        cache[n, v] <<- res
        res
    }
    
    k <- max_value
    while (min_wt(num_items, k) > capacity) k <- k - 1
    upper_bound <- k; lower_bound <- k
    pick <- vector("logical", num_items)
    
    for (n in seq(from = num_items - 1, to = 0)) {
        if (min_wt(n, k) != min_wt(n + 1, k)) {
            pick[n + 1] <- TRUE
            k <- k - value[n + 1]
        }
    }
    list(lower_bound = lower_bound, upper_bound = upper_bound,
         solution = take_items(ks, items$id[pick]))
}
```

And now I do see that I get the same solution as in the previous section:

```{r}
exact_solution(ks)
```

If we have an exact solution, why bother with branching-and-bounding? The dimensions of the `cache` matrix in `exact_solution` provide a hint: `exact_solution` will require space and time proportional to the number of items times the `max_value`. `max_value`, meanwhile, just depends on the values of the items in the knapsack. We can see this clearly by comparing execution time between two knapsack instances that are essentially the same, but where one has larger values. 

```{r}
ks1 <- subset_sum_instance(n = 10)
ks2 <- knapsack(capacity = capacity(ks1),
                items = items(ks1) %>% mutate(value = value * 10))
microbenchmark::microbenchmark(exact_solution(ks1), 
                               exact_solution(ks2), 
                               times = 10)
```

So, `exact_solution` will not scale very well. However, we can use it to build a better approximation than `greedy`.

## Better approximations during branch-and-bound

In the previous section we saw an algorithm that provides the exact optimal solution to the knapsack problem, but it does not scale well. In particular, it gets much slower as the items increase in value, even if nothing else about the instance changes.

But what would happen if we just scaled the values down? As it turns out, that is the idea behind a [fully polynomial time approximation scheme](https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme) for the knapsack! If we scale all of the items in a clever way, it can be shown that the solution attained by running the `exact_solution` from the previous section on the scaled-down instance will be close to the true optimal solution of the unscaled instance. Specifically, we can get to within $1 - \epsilon$ where $\epsilon$ is a parameter. 

```{r}
fptas <- function(ks, epsilon) {
    items <- items(ks)
    largest_value <- max(items$value)
    scaling_factor <- epsilon * (largest_value / n_items(ks))
    
    new_values <- floor(items$value / scaling_factor)
    new_items <- items
    new_items$value <- new_values
    
    new_ks <- knapsack(capacity = capacity(ks), items = new_items)
    approximate_solution <- taken_items(exact_solution(new_ks)$solution)
    if (nrow(approximate_solution) > 0) {
        approximate_solution <- take_items(ks, approximate_solution$id) 
    } else {
        approximate_solution <- ks
    }
    lower_bound <- total_value(approximate_solution)
    upper_bound <- lower_bound / (1 - epsilon)
    list(solution = approximate_solution,
         lower_bound = lower_bound,
         upper_bound = min(upper_bound, greedy(ks)$upper_bound))
}
```

Notice that `fptas` gives us a tighter lower bound than `greedy`:

```{r}
lower_bound(greedy(ks2)) 
lower_bound(fptas(ks2, epsilon = .3))
```

And `fptas` is also much faster than `exact_solution`:

```{r}
microbenchmark::microbenchmark(
    exact_solution(ks2), 
    fptas(ks2, epsilon = .3), 
    times = 10)
```

```{r}
ks <- strongly_correlated_instance(n = 20, R = 5000)
sol <- solve_knapsack(ks, bounds = function(x) ftpas(x, .3), 
                      search_strategy = best_first)

sola <- solve_knapsack(ks, bounds = greedy, search_strategy = best_first)
sol_ind <- stream_which(sol, function(x) n_items(x$problem) == 0)[1]
sola_ind <- stream_which(sola, function(x) n_items(x$problem) == 0)[1]


sol_df <- data_frame(
    timestamp = seq_len(sol_ind),
    lower_bound = stream_map(sol, lower_bound) %>% 
        as.list(from = 1, to = sol_ind) %>% as.numeric,
    upper_bound = stream_map(sol, upper_bound) %>%
        as.list(from = 1, to = sol_ind) %>% as.numeric) %>%
    mutate(best_seen = cummax(lower_bound))

sola_df <- data_frame(
    timestamp = seq_len(sola_ind),
    lower_bound = stream_map(sola, lower_bound) %>% 
        as.list(from = 1, to = sola_ind) %>% as.numeric,
    upper_bound = stream_map(sola, upper_bound) %>%
        as.list(from = 1, to = sola_ind) %>% as.numeric) %>%
    mutate(best_seen = cummax(lower_bound))

sol_df %>%
    mutate(method = "approxi") %>%
    bind_rows(sola_df %>% mutate(method = "greedy")) %>%
    select(-lower_bound) %>%
    gather(series, value, -timestamp, -method) %>%
    ggplot(aes(x = timestamp, y = value)) +
    geom_line(aes(linetype = series)) +
    geom_hline(yintercept = total_value(sola[sola_ind]$problem),
               colour = "red", linetype = 4) +
    scale_y_continuous(labels = scales::comma) + facet_wrap(~method)

sol_df %>%
    mutate(method = "approxi") %>%
    bind_rows(sola_df %>% mutate(method = "greedy")) %>%
    mutate(pct_of_upper_bound = best_seen / upper_bound) %>%
    ggplot(aes(x = timestamp, y = pct_of_upper_bound, colour = method)) +
    geom_line()

```
